{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "65adb871",
      "metadata": {},
      "source": [
        "## 1. Import thư viện, chuẩn bị dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36ae0bbc",
      "metadata": {
        "id": "36ae0bbc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "import shutil\n",
        "from os import makedirs\n",
        "from os.path import exists\n",
        "import re\n",
        "import string\n",
        "import struct\n",
        "from copy import deepcopy\n",
        "import copy\n",
        "import random\n",
        "from random import shuffle\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from numpy.random import random as rand\n",
        "import math\n",
        "import pickle\n",
        "import torch\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aa7c8043",
      "metadata": {
        "id": "aa7c8043"
      },
      "outputs": [],
      "source": [
        "class CommonConfigs(object):\n",
        "    def __init__(self, d_type):\n",
        "        self.ROOT_PATH = os.getcwd() + \"/\"  # Lấy đường dẫn thư mục hiện tại\n",
        "        self.TRAINING_DATA_PATH = self.ROOT_PATH + d_type + \"/train_set/\"  # Đường dẫn tập huấn luyện\n",
        "        self.VALIDATE_DATA_PATH = self.ROOT_PATH + d_type + \"/validate_set/\"  # Đường dẫn tập validation\n",
        "        self.TESTING_DATA_PATH = self.ROOT_PATH + d_type + \"/test_set/\"  # Đường dẫn tập kiểm tra\n",
        "        self.RESULT_PATH = self.ROOT_PATH + d_type + \"/result/\"  # Đường dẫn thư mục lưu kết quả\n",
        "        self.MODEL_PATH = self.ROOT_PATH + d_type + \"/model/\"  # Đường dẫn thư mục lưu mô hình\n",
        "        self.BEAM_SUMM_PATH = self.RESULT_PATH + \"/beam_summary/\"  # Đường dẫn lưu kết quả beam search\n",
        "        self.BEAM_GT_PATH = self.RESULT_PATH + \"/beam_ground_truth/\"  # Đường dẫn lưu ground truth beam search\n",
        "        self.GROUND_TRUTH_PATH = self.RESULT_PATH + \"/ground_truth/\"  # Đường dẫn ground truth\n",
        "        self.SUMM_PATH = self.RESULT_PATH + \"/summary/\"  # Đường dẫn lưu tóm tắt\n",
        "        self.TMP_PATH = self.ROOT_PATH + d_type + \"/tmp/\"  # Đường dẫn thư mục tạm\n",
        "\n",
        "class DeepmindTraining(object):\n",
        "    IS_UNICODE = False  # Setup sử dụng Unicode không\n",
        "    REMOVES_PUNCTION = False  # Loại bỏ dấu câu hay không\n",
        "    HAS_Y = True  # Có nhãn Y hay không\n",
        "    BATCH_SIZE = 32  # Kích thước batch\n",
        "\n",
        "class DeepmindTesting(object):\n",
        "    IS_UNICODE = False  # Setup sử dụng Unicode không\n",
        "    HAS_Y = True  # Có nhãn Y hay không\n",
        "    BATCH_SIZE = 100  # Kích thước batch\n",
        "    MIN_LEN_PREDICT = 35  # Độ dài tối thiểu của đầu ra dự đoán\n",
        "    MAX_LEN_PREDICT = 120  # Độ dài tối đa của đầu ra dự đoán\n",
        "    MAX_BYTE_PREDICT = None  # Giới hạn số byte đầu ra (None là không giới hạn)\n",
        "    PRINT_SIZE = 500  # Số lượng mẫu hiển thị khi in kết quả\n",
        "    REMOVES_PUNCTION = False  # Loại bỏ dấu câu hay không\n",
        "\n",
        "class DeepmindConfigs():\n",
        "    cc = CommonConfigs(\"so\")  # Cấu hình chung cho tập dữ liệu \"so\"\n",
        "\n",
        "    CELL = \"gru\"  # Loại cell RNN sử dụng (GRU hoặc LSTM)\n",
        "    CUDA = True  # Sử dụng GPU hay không\n",
        "    COPY = True  # Sử dụng cơ chế Copy hay không\n",
        "    COVERAGE = True  # Sử dụng cơ chế Coverage hay không\n",
        "    BI_RNN = True  # Sử dụng RNN hai chiều hay không\n",
        "    BEAM_SEARCH = True  # Sử dụng Beam Search hay không\n",
        "    BEAM_SIZE = 4  # Kích thước Beam Search\n",
        "    AVG_NLL = False  # Sử dụng trung bình Negative Log-Likelihood hay không\n",
        "    NORM_CLIP = 2  # Giới hạn giá trị norm của gradient\n",
        "    if not AVG_NLL:\n",
        "        NORM_CLIP = 5  # Nếu không dùng trung bình NLL thì đặt giá trị norm_clip là 5\n",
        "    LR = 0.15  # Learning rate\n",
        "\n",
        "    DIM_X = 100  # Số chiều embedding đầu vào\n",
        "    DIM_Y = DIM_X  # Số chiều embedding đầu ra\n",
        "\n",
        "    MIN_LEN_X = 16  # Độ dài tối thiểu của đầu vào\n",
        "    MIN_LEN_Y = 4  # Độ dài tối thiểu của đầu ra\n",
        "    MAX_LEN_X = 128  # Độ dài tối đa của đầu vào\n",
        "    MAX_LEN_Y = 64  # Độ dài tối đa của đầu ra\n",
        "    MIN_NUM_X = 1  # Số lượng tối thiểu của đầu vào\n",
        "    MAX_NUM_X = 1  # Số lượng tối đa của đầu vào\n",
        "    MAX_NUM_Y = None  # Số lượng tối đa của đầu ra (None là không giới hạn)\n",
        "\n",
        "    NUM_Y = 1  # Số lượng đầu ra\n",
        "    HIDDEN_SIZE = 256  # Kích thước hidden state của RNN\n",
        "\n",
        "    UNI_LOW_FREQ_THRESHOLD = 10  # Ngưỡng tần suất thấp cho từ vựng\n",
        "\n",
        "    PG_DICT_SIZE = 60000  # Kích thước từ điển của Pointer-Generator\n",
        "\n",
        "    W_UNK = \"<unk>\"  # Token unknown\n",
        "    W_BOS = \"<bos>\"  # Token bắt đầu câu\n",
        "    W_EOS = \"<eos>\"  # Token kết thúc câu\n",
        "    W_PAD = \"<pad>\"  # Token padding\n",
        "    W_LS = \"<s>\"  # Token bắt đầu câu (có thể dùng riêng)\n",
        "    W_RS = \"</s>\"  # Token kết thúc câu (có thể dùng riêng)\n",
        "\n",
        "class SOConfigs():\n",
        "    cc = CommonConfigs(\"so\")  # Cấu hình chung cho tập dữ liệu \"so\"\n",
        "\n",
        "    CELL = \"gru\"  # Loại cell RNN sử dụng (GRU hoặc LSTM)\n",
        "    CUDA = True  # Sử dụng GPU hay không\n",
        "    COPY = True  # Sử dụng cơ chế Copy hay không\n",
        "    COVERAGE = True  # Sử dụng cơ chế Coverage hay không\n",
        "    BI_RNN = True  # Sử dụng RNN hai chiều hay không\n",
        "    BEAM_SEARCH = True  # Sử dụng Beam Search hay không\n",
        "    BEAM_SIZE = 5  # Kích thước Beam Search\n",
        "    AVG_NLL = False  # Sử dụng trung bình Negative Log-Likelihood hay không\n",
        "    NORM_CLIP = 2  # Giới hạn giá trị norm của gradient\n",
        "    if not AVG_NLL:\n",
        "        NORM_CLIP = 5  # Nếu không dùng trung bình NLL thì đặt giá trị norm_clip là 5\n",
        "    LR = 0.15  # Learning rate\n",
        "\n",
        "    DIM_X = 100  # Số chiều embedding đầu vào\n",
        "    DIM_Y = DIM_X  # Số chiều embedding đầu ra\n",
        "\n",
        "    MIN_LEN_X = 16  # Độ dài tối thiểu của đầu vào\n",
        "    MIN_LEN_Y = 4  # Độ dài tối thiểu của đầu ra\n",
        "    MAX_LEN_X = 128  # Độ dài tối đa của đầu vào\n",
        "    MAX_LEN_Y = 64  # Độ dài tối đa của đầu ra\n",
        "    MIN_NUM_X = 1  # Số lượng tối thiểu của đầu vào\n",
        "    MAX_NUM_X = 1  # Số lượng tối đa của đầu vào\n",
        "    MAX_NUM_Y = None  # Số lượng tối đa của đầu ra (None là không giới hạn)\n",
        "\n",
        "    NUM_Y = 1  # Số lượng đầu ra\n",
        "    HIDDEN_SIZE = 256  # Kích thước hidden state của RNN\n",
        "\n",
        "    UNI_LOW_FREQ_THRESHOLD = 10  # Ngưỡng tần suất thấp cho từ vựng\n",
        "\n",
        "    PG_DICT_SIZE = 50000  # Kích thước từ điển của Pointer-Generator\n",
        "\n",
        "    W_UNK = \"<unk>\"  # Token unknown\n",
        "    W_BOS = \"<bos>\"  # Token bắt đầu câu\n",
        "    W_EOS = \"<eos>\"  # Token kết thúc câu\n",
        "    W_PAD = \"<pad>\"  # Token padding\n",
        "    W_LS = \"<s>\"  # Token bắt đầu câu (có thể dùng riêng)\n",
        "    W_RS = \"</s>\"  # Token kết thúc câu (có thể dùng riêng)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "df35de79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df35de79",
        "outputId": "ea205a80-7b80-46a9-cd90-b465a5cd7989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: c:\\Users\\Acer\\Desktop\\2024\\lab\\Code2que/so/train_set/\n",
            "test: c:\\Users\\Acer\\Desktop\\2024\\lab\\Code2que/so/test_set/\n",
            "validate: c:\\Users\\Acer\\Desktop\\2024\\lab\\Code2que/so/validate_set/\n",
            "result: c:\\Users\\Acer\\Desktop\\2024\\lab\\Code2que/so/result/\n",
            "model: c:\\Users\\Acer\\Desktop\\2024\\lab\\Code2que/so/model/\n",
            "tmp: c:\\Users\\Acer\\Desktop\\2024\\lab\\Code2que/so/tmp/\n",
            "Train set size: 200566\n",
            "Validation set size: 50142\n",
            "fitering and building dict...\n"
          ]
        }
      ],
      "source": [
        "stop_words = {\"-lrb-\", \"-rrb-\", \"-\"}\n",
        "unk_words = {\"unk\", \"<unk>\"}\n",
        "\n",
        "def load_vocab(src_path, tgt_path):\n",
        "    \"\"\"\n",
        "    Đọc từ vựng từ file nguồn và file đích, tạo một từ điển đếm số lần xuất hiện của từng từ.\n",
        "    \"\"\"\n",
        "    vocab_dict = {}\n",
        "    vocab_list = []\n",
        "    with open(src_path, 'r') as src:\n",
        "        for line in src:\n",
        "            words = line.lower().split()\n",
        "            for word in words:\n",
        "                if word not in vocab_dict:\n",
        "                    vocab_dict[word] = 1\n",
        "                    vocab_list.append(word)\n",
        "                else:\n",
        "                    vocab_dict[word] += 1\n",
        "\n",
        "    with open(tgt_path, 'r') as src:\n",
        "        for line in src:\n",
        "            words = line.lower().split()\n",
        "            for word in words:\n",
        "                if word not in vocab_dict:\n",
        "                    vocab_dict[word] = 1\n",
        "                    vocab_list.append(word)\n",
        "                else:\n",
        "                    vocab_dict[word] += 1\n",
        "    return vocab_dict, vocab_list\n",
        "    pass\n",
        "\n",
        "def to_dict(xys, dic):\n",
        "    # Tập từ điển không bao gồm tập kiếm tra (dict should not consider test set)!!!!!\n",
        "    \"\"\"\n",
        "    Cập nhật từ điển từ vựng với dữ liệu từ tập huấn luyện.\n",
        "    \"\"\"\n",
        "    for xy in xys:\n",
        "        sents, summs = xy\n",
        "        y = summs[0]\n",
        "        for w in y:\n",
        "            if w in dic:\n",
        "                dic[w] += 1\n",
        "            else:\n",
        "                dic[w] = 1\n",
        "\n",
        "        x = sents[0]\n",
        "        for w in x:\n",
        "            if w in dic:\n",
        "                dic[w] += 1\n",
        "            else:\n",
        "                dic[w] = 1\n",
        "    return dic\n",
        "\n",
        "def prepare_dict(src_path, tgt_path, train_xy_list, configs):\n",
        "    \"\"\"\n",
        "    Xây dựng từ điển từ vựng với giới hạn số lượng từ nhất định.\n",
        "    \"\"\"\n",
        "    print (\"fitering and building dict...\")\n",
        "    use_abisee = True\n",
        "    all_dic1 = {}\n",
        "    all_dic2 = {}\n",
        "    dic_list = []\n",
        "    all_dic1, dic_list = load_vocab(src_path, tgt_path)\n",
        "    all_dic2 = to_dict(train_xy_list, all_dic2)\n",
        "    for w, tf in all_dic2.items():\n",
        "        if w not in all_dic1:\n",
        "            all_dic1[w] = tf\n",
        "\n",
        "    candiate_list = dic_list[0:configs.PG_DICT_SIZE] # 50000\n",
        "    candiate_set = set(candiate_list)\n",
        "\n",
        "    dic = {}\n",
        "    w2i = {}\n",
        "    i2w = {}\n",
        "    w2w = {}\n",
        "\n",
        "    for w in [configs.W_PAD, configs.W_UNK, configs.W_EOS]:\n",
        "        w2i[w] = len(dic)\n",
        "        i2w[w2i[w]] = w\n",
        "        dic[w] = 10000\n",
        "        w2w[w] = w\n",
        "\n",
        "    for w, tf in all_dic1.items():\n",
        "        if w in candiate_set:\n",
        "            w2i[w] = len(dic)\n",
        "            i2w[w2i[w]] = w\n",
        "            dic[w] = tf\n",
        "            w2w[w] = w\n",
        "        else:\n",
        "            w2w[w] = configs.W_UNK\n",
        "\n",
        "    hfw = []\n",
        "    sorted_x = sorted(dic.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for w in sorted_x:\n",
        "        hfw.append(w[0])\n",
        "\n",
        "    assert len(hfw) == len(dic)\n",
        "    assert len(w2i) == len(dic)\n",
        "    return all_dic1, dic, hfw, w2i, i2w, w2w\n",
        "\n",
        "\n",
        "def get_xy_tuple(cont, head, configs):\n",
        "    \"\"\"\n",
        "    Chuyển đổi nội dung từ câu thành tuple (đoạn văn bản gốc, câu tóm tắt).\n",
        "    \"\"\"\n",
        "    x = read_cont(cont, configs)\n",
        "    y = read_head(head, configs)\n",
        "\n",
        "    if x != None and y != None:\n",
        "        return (x, y)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def read_cont(src_line, cfg):\n",
        "    \"\"\"\n",
        "    Xử lý văn bản nguồn (đoạn văn gốc) và thêm token kết thúc câu.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    line = src_line #del_num(f_cont)\n",
        "    words = line.split()\n",
        "    lines += words\n",
        "    lines += [cfg.W_EOS]\n",
        "    return (lines, src_line)\n",
        "\n",
        "def read_head(tgt_line, cfg):\n",
        "    \"\"\"\n",
        "    Xử lý tiêu đề hoặc câu tóm tắt với token kết thúc câu.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    line = tgt_line\n",
        "    words = line.split()\n",
        "    lines += words\n",
        "    lines += [cfg.W_EOS]\n",
        "    return (lines, tgt_line)\n",
        "\n",
        "def load_lines(src_path, tgt_path,  configs):\n",
        "    \"\"\"\n",
        "    Đọc dữ liệu từ file nguồn và file đích, sau đó chuyển đổi thành danh sách tuple (input, summary).\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    with open(src_path, 'r') as src_file, open(tgt_path, 'r') as tgt_file:\n",
        "        for src_line, tgt_line in zip(src_file, tgt_file):\n",
        "\n",
        "            src_line = src_line.strip().lower()\n",
        "            tgt_line = tgt_line.strip().lower()\n",
        "            xy_tuple = get_xy_tuple(src_line, tgt_line, configs)\n",
        "            lines.append(xy_tuple)\n",
        "    return lines\n",
        "\n",
        "\n",
        "def prepare_dir():\n",
        "   return configs\n",
        "\n",
        "\"\"\"\n",
        "Chuẩn bị và xử lý dữ liệu tập huấn luyện, kiểm thử, và validation.\n",
        "\"\"\"\n",
        "# Prepare Dirs\n",
        "configs = DeepmindConfigs()\n",
        "TRAINING_PATH = configs.cc.TRAINING_DATA_PATH\n",
        "VALIDATE_PATH = configs.cc.VALIDATE_DATA_PATH\n",
        "TESTING_PATH = configs.cc.TESTING_DATA_PATH\n",
        "RESULT_PATH = configs.cc.RESULT_PATH\n",
        "MODEL_PATH = configs.cc.MODEL_PATH\n",
        "BEAM_SUMM_PATH = configs.cc.BEAM_SUMM_PATH\n",
        "BEAM_GT_PATH = configs.cc.BEAM_GT_PATH\n",
        "GROUND_TRUTH_PATH = configs.cc.GROUND_TRUTH_PATH\n",
        "SUMM_PATH = configs.cc.SUMM_PATH\n",
        "TMP_PATH = configs.cc.TMP_PATH\n",
        "\n",
        "print (\"train: \" + TRAINING_PATH)\n",
        "print (\"test: \" + TESTING_PATH)\n",
        "print (\"validate: \" + VALIDATE_PATH)\n",
        "print (\"result: \" + RESULT_PATH)\n",
        "print (\"model: \" + MODEL_PATH)\n",
        "print (\"tmp: \" + TMP_PATH)\n",
        "\n",
        "if not exists(TRAINING_PATH):\n",
        "    makedirs(TRAINING_PATH)\n",
        "if not exists(VALIDATE_PATH):\n",
        "    makedirs(VALIDATE_PATH)\n",
        "if not exists(TESTING_PATH):\n",
        "    makedirs(TESTING_PATH)\n",
        "if not exists(RESULT_PATH):\n",
        "    makedirs(RESULT_PATH)\n",
        "if not exists(MODEL_PATH):\n",
        "    makedirs(MODEL_PATH)\n",
        "if not exists(BEAM_SUMM_PATH):\n",
        "    makedirs(BEAM_SUMM_PATH)\n",
        "if not exists(BEAM_GT_PATH):\n",
        "    makedirs(BEAM_GT_PATH)\n",
        "if not exists(GROUND_TRUTH_PATH):\n",
        "    makedirs(GROUND_TRUTH_PATH)\n",
        "if not exists(SUMM_PATH):\n",
        "    makedirs(SUMM_PATH)\n",
        "if not exists(TMP_PATH):\n",
        "    makedirs(TMP_PATH)\n",
        "\n",
        "data_dir = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\Code2que\\Challenge Task 2 - Code2Que\\data\\archive\\Code2Que-data\\javadata\"\n",
        "# Prepare Dataset\n",
        "src_path = f\"{data_dir}/src-train.txt\"\n",
        "tgt_path = f\"{data_dir}/tgt-train.txt\"\n",
        "\n",
        "#train\n",
        "train_xy_list = load_lines(src_path, tgt_path,  configs)\n",
        "\n",
        "train_xy_list, val_xy_list = train_test_split(train_xy_list, test_size=0.2, random_state=42)\n",
        "print(f\"Train set size: {len(train_xy_list)}\")\n",
        "print(f\"Validation set size: {len(val_xy_list)}\")\n",
        "#dictionary\n",
        "all_dic1, dic, hfw, w2i, i2w, w2w = prepare_dict(src_path, tgt_path, train_xy_list, configs)\n",
        "\n",
        "#test\n",
        "src_path = f\"{data_dir}/src-test-clear.txt\"\n",
        "tgt_path = f\"{data_dir}/tgt-test-clear.txt\"\n",
        "test_xy_list = load_lines(src_path, tgt_path,  configs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "957c17e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "957c17e3",
        "outputId": "c22660ca-e7e8-4313-8e30-5eb49a1928bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cấu trúc của một cặp (x, y):\n",
            "x (source code): ['long', 'starttime', '=', 'system', '.', 'currenttimemillis', '();', 'while', '(', 'running', ')', '{', 'track', '.', 'repaint', '();', '//', 'varying', 'the', 'x']... (đã cắt ngắn)\n",
            "y (question): ['how', 'to', 'round', 'seconds', 'to', 'tenths', 'place', '?', '<eos>']... (đã cắt ngắn)\n",
            "\n",
            "Đoạn code đầy đủ:\n",
            "long starttime = system . currenttimemillis (); while ( running ) { track . repaint (); // varying the x position movement horse . setx ( xpos += ( int ) ( math . random () * num + num )); // sleeping the thread try { thread . sleep ( 100 ); } catch ( interruptedexception e ) { e . printstacktrace (); } if ( xpos >= finish_line ) { running = false ; long endtime = system . currenttimemillis (); joptionpane . showmessagedialog ( new jframe (), id + str +( float )( starttime - endtime )/ 1000 *- 1 . 0 + str ); } }\n",
            "\n",
            "Câu hỏi đầy đủ:\n",
            "how to round seconds to tenths place ?\n"
          ]
        }
      ],
      "source": [
        "# Lấy một cặp từ train_xy_list\n",
        "sample_pair = train_xy_list[0]\n",
        "\n",
        "# In ra cấu trúc của cặp dữ liệu\n",
        "print(\"Cấu trúc của một cặp (x, y):\")\n",
        "print(f\"x (source code): {sample_pair[0][0][:20]}... (đã cắt ngắn)\")\n",
        "print(f\"y (question): {sample_pair[1][0][:20]}... (đã cắt ngắn)\")\n",
        "\n",
        "# In ra văn bản gốc đầy đủ\n",
        "print(\"\\nĐoạn code đầy đủ:\")\n",
        "print(sample_pair[0][1])\n",
        "\n",
        "# In ra câu hỏi đầy đủ\n",
        "print(\"\\nCâu hỏi đầy đủ:\")\n",
        "print(sample_pair[1][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09a3981",
      "metadata": {},
      "source": [
        "## 2. Chuẩn bị mô hình CodeBERT(encoder) - LSTM + Attention (decoder)\n",
        "- Encoder: Biến code thành token ID (1, sentence_len) -> Embedding (1, sentence_len, 768) -> CodeBERT layers -> Output là last hidden state (1, sentenece_len, 768)\n",
        "- Decoder: Nhúng token đầu vào thành vector -> Tính attention score với h_t và encoder output -> Context vector -> Ghép với cả embedding đầu vào cho LSTM (Từ trước) -> Tính xác suất sinh từ trên từ điển (lấy từ tập các câu hỏi) dùng greedy search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "75628c4c",
      "metadata": {
        "id": "75628c4c"
      },
      "outputs": [],
      "source": [
        "# CodeBERT LSTM Attention Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class CodeBERTEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder sử dụng mô hình CodeBERT từ Microsoft Research\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_encoder=False):\n",
        "        super(CodeBERTEncoder, self).__init__()\n",
        "        self.codebert = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "        # Cố định các tham số của CodeBERT nếu cần\n",
        "        if freeze_encoder:\n",
        "            for param in self.codebert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        :param input_ids: Token IDs của đoạn code [batch_size, seq_length]\n",
        "        :param attention_mask: Attention mask [batch_size, seq_length]\n",
        "        :return: Tất cả hidden states từ CodeBERT\n",
        "        \"\"\"\n",
        "        outputs = self.codebert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Lấy tất cả các hidden states\n",
        "        hidden_states = outputs.last_hidden_state  # [batch_size, seq_length, hidden_size]\n",
        "        return hidden_states\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Cơ chế Attention dựa trên Bahdanau et al. (2015)\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_dim, decoder_dim):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.encoder_dim = encoder_dim  # Kích thước hidden của encoder (768)\n",
        "        self.decoder_dim = decoder_dim  # Kích thước hidden của decoder (256)\n",
        "\n",
        "        # Mạng fully connected để tính attention scores\n",
        "        self.attn = nn.Linear(self.encoder_dim + self.decoder_dim, self.decoder_dim)\n",
        "        self.v = nn.Linear(self.decoder_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_hidden, mask=None):\n",
        "        \"\"\"\n",
        "        :param encoder_outputs: Outputs của encoder [batch_size, src_seq_len, encoder_dim]\n",
        "        :param decoder_hidden: Hidden state của decoder [batch_size, decoder_dim]\n",
        "        :param mask: Mask cho padding tokens [batch_size, src_seq_len]\n",
        "        \"\"\"\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "        # Mở rộng decoder_hidden để kết hợp với từng token đầu ra của encoder\n",
        "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, decoder_dim]\n",
        "\n",
        "        # Concatenate encoder outputs và decoder hidden state\n",
        "        attn_inputs = torch.cat((decoder_hidden, encoder_outputs), dim=2)  # [batch_size, src_len, encoder_dim + decoder_dim]\n",
        "\n",
        "        # Tính attention scores\n",
        "        energy = torch.tanh(self.attn(attn_inputs))  # [batch_size, src_len, decoder_dim]\n",
        "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
        "\n",
        "        # Áp dụng mask nếu có\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # Chuyển thành phân phối xác suất qua softmax\n",
        "        attention_weights = F.softmax(attention, dim=1)  # [batch_size, src_len]\n",
        "\n",
        "        # Tính weighted sum\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, encoder_dim]\n",
        "        context_vector = context_vector.squeeze(1)  # [batch_size, encoder_dim]\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder sử dụng LSTM với cơ chế attention\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embed_dim,\n",
        "                 encoder_dim=768,\n",
        "                 hidden_dim=256,\n",
        "                 num_layers=2,\n",
        "                 dropout=0.5):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size  # Kích thước từ điển đầu ra\n",
        "        self.embed_dim = embed_dim    # Dimension của word embeddings\n",
        "        self.encoder_dim = encoder_dim  # Dimension của encoder outputs (768 cho CodeBERT)\n",
        "        self.hidden_dim = hidden_dim  # Dimension của decoder hidden state\n",
        "        self.num_layers = num_layers  # Số lớp LSTM\n",
        "\n",
        "        # Embedding layer cho từ đầu ra\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Cơ chế attention\n",
        "        self.attention = BahdanauAttention(encoder_dim, hidden_dim)\n",
        "\n",
        "        # Projection layer để giảm kích thước của encoder outputs\n",
        "        self.encoder_projection = nn.Linear(encoder_dim, hidden_dim)\n",
        "\n",
        "        # LSTM decoder\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim + encoder_dim,  # Đầu vào là concatenation của embedding và context vector\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Lớp output projection\n",
        "        self.output_projection = nn.Linear(hidden_dim + encoder_dim, vocab_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def init_hidden(self, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Khởi tạo hidden state của LSTM từ encoder outputs\n",
        "        \"\"\"\n",
        "        # Sử dụng CLS token (vị trí 0) để khởi tạo\n",
        "        cls_token = encoder_outputs[:, 0, :]  # [batch_size, encoder_dim]\n",
        "\n",
        "        # Project xuống kích thước hidden của decoder\n",
        "        h0 = self.encoder_projection(cls_token)\n",
        "        h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)  # [num_layers, batch_size, hidden_dim]\n",
        "        c0 = torch.zeros_like(h0)\n",
        "\n",
        "        return (h0, c0)\n",
        "\n",
        "    def forward_step(self, input_token, last_hidden, encoder_outputs, src_mask=None):\n",
        "        \"\"\"\n",
        "        Một bước decoder cho một token đầu vào\n",
        "        \"\"\"\n",
        "        # Input token embedding [batch_size, 1, embed_dim]\n",
        "        embedded = self.embedding(input_token)\n",
        "\n",
        "        # Lấy ra hidden state gần nhất\n",
        "        h_n, c_n = last_hidden\n",
        "        decoder_hidden = h_n[-1]  # Lấy hidden state của layer cuối cùng [batch_size, hidden_dim]\n",
        "\n",
        "        # Tính attention context vector\n",
        "        context_vector, attention_weights = self.attention(encoder_outputs, decoder_hidden, src_mask)\n",
        "\n",
        "        # Concatenate embedding và context vector\n",
        "        lstm_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=2)\n",
        "\n",
        "        # LSTM forward\n",
        "        output, new_hidden = self.lstm(lstm_input, last_hidden)\n",
        "\n",
        "        # Concatenate output với context vector\n",
        "        output = torch.cat((output.squeeze(1), context_vector), dim=1)  # [batch_size, hidden_dim + encoder_dim]\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        # Final projection\n",
        "        output = self.output_projection(output)  # [batch_size, vocab_size]\n",
        "\n",
        "        return output, new_hidden, attention_weights\n",
        "\n",
        "    def forward(self, encoder_outputs, target_tokens, src_mask=None, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass đầy đủ của decoder\n",
        "        :param encoder_outputs: Outputs từ encoder [batch_size, src_seq_len, encoder_dim]\n",
        "        :param target_tokens: Target sequence để decode [batch_size, tgt_seq_len]\n",
        "        :param src_mask: Mask cho source sequence [batch_size, src_seq_len]\n",
        "        :param teacher_forcing_ratio: Tỷ lệ sử dụng teacher forcing\n",
        "        \"\"\"\n",
        "        batch_size, target_length = target_tokens.size()\n",
        "\n",
        "        # Khởi tạo hidden state từ encoder outputs\n",
        "        hidden = self.init_hidden(encoder_outputs)\n",
        "\n",
        "        # Chuẩn bị tensor để lưu các dự đoán\n",
        "        outputs = torch.zeros(batch_size, target_length, self.vocab_size).to(target_tokens.device)\n",
        "\n",
        "        # Token đầu tiên là BOS (Beginning of Sequence)\n",
        "        input_token = target_tokens[:, 0].unsqueeze(1)  # [batch_size, 1]\n",
        "\n",
        "        # Lặp qua từng timestep\n",
        "        for t in range(1, target_length):\n",
        "            # Forward step cho một token\n",
        "            output, hidden, _ = self.forward_step(input_token, hidden, encoder_outputs, src_mask)\n",
        "\n",
        "            # Lưu dự đoán\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # Teacher forcing\n",
        "            use_teacher_forcing = (torch.rand(1).item() < teacher_forcing_ratio)\n",
        "\n",
        "            if use_teacher_forcing:\n",
        "                # Sử dụng target token làm input tiếp theo\n",
        "                input_token = target_tokens[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                # Sử dụng dự đoán làm input tiếp theo\n",
        "                _, top_indices = output.topk(1)\n",
        "                input_token = top_indices\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, encoder_outputs, src_mask=None, max_length=64, bos_id=0, eos_id=2):\n",
        "        \"\"\"\n",
        "        Sinh câu từ encoder outputs (inference)\n",
        "        \"\"\"\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "\n",
        "        # Khởi tạo hidden state\n",
        "        hidden = self.init_hidden(encoder_outputs)\n",
        "\n",
        "        # Bắt đầu với token BOS\n",
        "        input_token = torch.LongTensor([[bos_id]] * batch_size).to(encoder_outputs.device)\n",
        "\n",
        "        # Chuẩn bị tensors để lưu tokens và attention weights\n",
        "        generated_tokens = []\n",
        "        attention_weights_list = []\n",
        "\n",
        "        # Sinh tokens cho đến khi đạt max_length hoặc gặp EOS\n",
        "        for _ in range(max_length):\n",
        "            # Forward step\n",
        "            output, hidden, attention_weights = self.forward_step(input_token, hidden, encoder_outputs, src_mask)\n",
        "\n",
        "            # Lấy token với xác suất cao nhất\n",
        "            _, top_indices = output.topk(1)\n",
        "\n",
        "            # Lưu token và attention weights\n",
        "            generated_tokens.append(top_indices)\n",
        "            attention_weights_list.append(attention_weights)\n",
        "\n",
        "            # Input cho bước tiếp theo\n",
        "            input_token = top_indices\n",
        "\n",
        "            # Kiểm tra nếu tất cả batch đã sinh EOS token\n",
        "            if (top_indices == eos_id).all():\n",
        "                break\n",
        "\n",
        "        # Stack all tokens and attention weights\n",
        "        tokens = torch.cat(generated_tokens, dim=1)  # [batch_size, seq_len]\n",
        "        attention_matrix = torch.stack(attention_weights_list, dim=1)  # [batch_size, seq_len, src_len]\n",
        "\n",
        "        return tokens, attention_matrix\n",
        "\n",
        "class CodeBERTLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Mô hình encoder-decoder kết hợp CodeBERT và LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embed_dim=256,\n",
        "                 decoder_hidden_dim=256,\n",
        "                 decoder_layers=2,\n",
        "                 dropout=0.5,\n",
        "                 freeze_encoder=False):\n",
        "        super(CodeBERTLSTM, self).__init__()\n",
        "\n",
        "        self.encoder = CodeBERTEncoder(freeze_encoder=freeze_encoder)\n",
        "        self.decoder = LSTMDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            embed_dim=embed_dim,\n",
        "            encoder_dim=768,  # CodeBERT hidden size\n",
        "            hidden_dim=decoder_hidden_dim,\n",
        "            num_layers=decoder_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, target_tokens, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass của mô hình encoder-decoder\n",
        "        \"\"\"\n",
        "        # Encoder\n",
        "        encoder_outputs = self.encoder(input_ids, attention_mask)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_outputs = self.decoder(\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            target_tokens=target_tokens,\n",
        "            src_mask=attention_mask,\n",
        "            teacher_forcing_ratio=teacher_forcing_ratio\n",
        "        )\n",
        "\n",
        "        return decoder_outputs\n",
        "\n",
        "    def generate(self, input_ids, attention_mask, max_length=64, bos_id=0, eos_id=2):\n",
        "        \"\"\"\n",
        "        Sinh câu hỏi từ đoạn code\n",
        "        \"\"\"\n",
        "        # Encoder\n",
        "        encoder_outputs = self.encoder(input_ids, attention_mask)\n",
        "\n",
        "        # Decoder generate\n",
        "        generated_tokens, attention_matrix = self.decoder.generate(\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            src_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            bos_id=bos_id,\n",
        "            eos_id=eos_id\n",
        "        )\n",
        "\n",
        "        return generated_tokens, attention_matrix\n",
        "\n",
        "# Hàm tính loss\n",
        "def compute_loss(outputs, targets, pad_id=0):\n",
        "    \"\"\"\n",
        "    Tính cross-entropy loss, bỏ qua các padding tokens\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "    batch_size, seq_len, vocab_size = outputs.size()\n",
        "\n",
        "    # Reshape outputs và targets\n",
        "    outputs = outputs.view(-1, vocab_size)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    loss = criterion(outputs, targets)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8a062285",
      "metadata": {
        "id": "8a062285"
      },
      "outputs": [],
      "source": [
        "def train_batch(model, optimizer, batch, tokenizer, device, teacher_forcing_ratio=0.5):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Lấy code và câu hỏi từ batch\n",
        "    code_batch, question_batch = batch\n",
        "\n",
        "    # Tokenize đoạn code với CodeBERT tokenizer\n",
        "    code_inputs = tokenizer(\n",
        "        code_batch,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "\n",
        "    # Tokenize câu hỏi với target tokenizer\n",
        "    question_tokens = torch.tensor(question_batch).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(\n",
        "        input_ids=code_inputs.input_ids,\n",
        "        attention_mask=code_inputs.attention_mask,\n",
        "        target_tokens=question_tokens,\n",
        "        teacher_forcing_ratio=teacher_forcing_ratio\n",
        "    )\n",
        "\n",
        "    # Tính loss\n",
        "    loss = compute_loss(outputs, question_tokens, pad_id=0)  # Giả sử 0 là pad_id\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Cập nhật weights\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# Sử dụng mô hình cho inference\n",
        "def generate_question(model, code_snippet, tokenizer, question_tokenizer, device, max_length=64):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize đoạn code với CodeBERT tokenizer\n",
        "    code_inputs = tokenizer(\n",
        "        code_snippet,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "\n",
        "    # Sinh câu hỏi\n",
        "    with torch.no_grad():\n",
        "        generated_ids, attention_matrix = model.generate(\n",
        "            input_ids=code_inputs.input_ids,\n",
        "            attention_mask=code_inputs.attention_mask,\n",
        "            max_length=max_length,\n",
        "            bos_id=question_tokenizer.bos_token_id,\n",
        "            eos_id=question_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Chuyển đổi ids thành câu\n",
        "    generated_questions = question_tokenizer.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return generated_questions, attention_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e9c9da42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Small train set size: 100000\n",
            "Small validation set size: 1\n",
            "Small test set size: 100\n"
          ]
        }
      ],
      "source": [
        "train_xy_list_small = train_xy_list[:100000]\n",
        "val_xy_list_small = train_xy_list[51:52]  # Có thể dùng cùng mẫu vì chỉ đang test chạy được không\n",
        "test_xy_list_small = test_xy_list[:100]  # Chỉ lấy 5 mẫu test\n",
        "\n",
        "print(f\"Small train set size: {len(train_xy_list_small)}\")\n",
        "print(f\"Small validation set size: {len(val_xy_list_small)}\")\n",
        "print(f\"Small test set size: {len(test_xy_list_small)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93bcdf1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bổ sung các token đặc biệt vào từ điển...\n",
            "Đã thêm token '<s>' với ID 60003\n",
            "Đã thêm token '</s>' với ID 60004\n",
            "\n",
            "--- Kiểm tra từ điển ---\n",
            "<pad> -> ID 0 -> <pad> ✓\n",
            "<unk> -> ID 1 -> <unk> ✓\n",
            "<bos> -> NOT IN w2i ❌\n",
            "<eos> -> ID 2 -> <eos> ✓\n",
            "<s> -> ID 60003 -> <s> ✓\n",
            "</s> -> ID 60004 -> </s> ✓\n",
            "\n",
            "Kích thước w2i: 60005\n",
            "Kích thước i2w: 60005\n",
            "------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def check_dictionaries(w2i, i2w):\n",
        "    \"\"\"Kiểm tra tính nhất quán giữa các từ điển w2i và i2w\"\"\"\n",
        "    print(\"\\n--- Kiểm tra từ điển ---\")\n",
        "    \n",
        "    # Kiểm tra các token đặc biệt\n",
        "    special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<s>\", \"</s>\"]\n",
        "    for token in special_tokens:\n",
        "        if token in w2i:\n",
        "            token_id = w2i[token]\n",
        "            print(f\"{token} -> ID {token_id}\", end=\"\")\n",
        "            \n",
        "            if token_id in i2w:\n",
        "                print(f\" -> {i2w[token_id]} ✓\")\n",
        "            else:\n",
        "                print(f\" -> MISSING IN i2w ❌\")\n",
        "        else:\n",
        "            print(f\"{token} -> NOT IN w2i ❌\")\n",
        "    \n",
        "    print(f\"\\nKích thước w2i: {len(w2i)}\")\n",
        "    print(f\"Kích thước i2w: {len(i2w)}\")\n",
        "    print(\"------------------------\\n\")\n",
        "\n",
        "def add_special_tokens_to_dict(w2i, i2w):\n",
        "    \"\"\"Bổ sung các token đặc biệt <s> và </s> vào từ điển nếu chúng chưa tồn tại.\"\"\"\n",
        "    print(\"\\nBổ sung các token đặc biệt vào từ điển...\")\n",
        "    \n",
        "    # Thêm token <s>\n",
        "    if '<s>' not in w2i:\n",
        "        token_id = len(w2i)\n",
        "        w2i['<s>'] = token_id\n",
        "        i2w[token_id] = '<s>'\n",
        "        print(f\"Đã thêm token '<s>' với ID {token_id}\")\n",
        "    else:\n",
        "        print(f\"Token '<s>' đã tồn tại với ID {w2i['<s>']}\")\n",
        "    \n",
        "    # Thêm token </s>\n",
        "    if '</s>' not in w2i:\n",
        "        token_id = len(w2i)\n",
        "        w2i['</s>'] = token_id\n",
        "        i2w[token_id] = '</s>'\n",
        "        print(f\"Đã thêm token '</s>' với ID {token_id}\")\n",
        "    else:\n",
        "        print(f\"Token '</s>' đã tồn tại với ID {w2i['</s>']}\")\n",
        "    \n",
        "    # Kiểm tra lại sau khi thêm\n",
        "    check_dictionaries(w2i, i2w)\n",
        "    \n",
        "    return w2i, i2w\n",
        "\n",
        "w2i, i2w = add_special_tokens_to_dict(w2i, i2w)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f836185a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_sample_encoding():\n",
        "    # Lấy một mẫu từ tập huấn luyện\n",
        "    sample_pair = train_xy_list[0]\n",
        "    code_text = sample_pair[0][1]  # Lấy văn bản code gốc\n",
        "    question_text = sample_pair[1][1]  # Lấy câu hỏi gốc\n",
        "    question_tokens = sample_pair[1][0]  # Lấy tokens của câu hỏi\n",
        "    \n",
        "    print(\"=== KIỂM TRA MÃ HÓA MẪU VÀ ÁNH XẠ TỪ ĐIỂN ===\")\n",
        "    print(f\"\\n1. Mẫu gốc:\")\n",
        "    print(f\"   - Code: {code_text[:100]}...\")\n",
        "    print(f\"   - Question: {question_text}\")\n",
        "    \n",
        "    print(\"\\n2. Tokens của câu hỏi:\")\n",
        "    print(f\"   {question_tokens[:20]}...\")\n",
        "    \n",
        "    # Mã hóa câu hỏi thành chỉ số\n",
        "    question_indices = [w2i['<bos>']]  # Bắt đầu với BOS token\n",
        "    token_to_id = {}  # Lưu ánh xạ để kiểm tra\n",
        "    \n",
        "    print(\"\\n3. Mã hóa từng token trong câu hỏi:\")\n",
        "    print(f\"   {'Token':<15} | {'ID':<8} | {'Lookup lại từ i2w'}\")\n",
        "    print(f\"   {'-'*15} | {'-'*8} | {'-'*20}\")\n",
        "    \n",
        "    print(f\"   {'<bos>':<15} | {w2i['<bos>']:<8} | {i2w[w2i['<bos>']]} ({'✓' if i2w[w2i['<bos>']] == '<bos>' else '❌'})\")\n",
        "    \n",
        "    for token in question_tokens:\n",
        "        if token == \"<eos>\":\n",
        "            idx = w2i[\"<eos>\"]\n",
        "        elif token in w2i:\n",
        "            idx = w2i[token]\n",
        "        else:\n",
        "            idx = w2i[\"<unk>\"]\n",
        "        \n",
        "        question_indices.append(idx)\n",
        "        token_to_id[token] = idx\n",
        "        lookup = i2w[idx] if idx in i2w else \"MISSING IN i2w\"\n",
        "        check = \"✓\" if i2w[idx] == token else \"❌\"\n",
        "        \n",
        "        print(f\"   {token[:15]:<15} | {idx:<8} | {lookup} ({check})\")\n",
        "        \n",
        "        if len(question_indices) > 20:  # Giới hạn số lượng token hiển thị\n",
        "            print(\"   ... and more ...\")\n",
        "            break\n",
        "    \n",
        "    # Kiểm tra giải mã từ indices trở lại thành tokens\n",
        "    decoded_tokens = []\n",
        "    for idx in question_indices:\n",
        "        if idx in i2w:\n",
        "            token = i2w[idx]\n",
        "            decoded_tokens.append(token)\n",
        "    \n",
        "    print(\"\\n4. Giải mã chỉ số thành tokens:\")\n",
        "    print(f\"   {' '.join(decoded_tokens[:20])}...\")\n",
        "    \n",
        "    # Mã hóa sử dụng Code2QueDataset để so sánh\n",
        "    print(\"\\n5. Kiểm tra mã hóa từ Code2QueDataset:\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "    dataset = Code2QueDataset([sample_pair], w2i, tokenizer, i2w=i2w)\n",
        "    sample = dataset[0]\n",
        "    \n",
        "    print(f\"   - Shape của question_indices: {sample['question_indices'].shape}\")\n",
        "    \n",
        "    # So sánh một số phần tử đầu tiên của mảng indices\n",
        "    manually_encoded = question_indices[:10]\n",
        "    dataset_encoded = sample['question_indices'][:10].tolist()\n",
        "    \n",
        "    print(\"\\n6. So sánh kết quả mã hóa thủ công và từ Dataset:\")\n",
        "    print(f\"   {'Index':<5} | {'Thủ công':<10} | {'Dataset':<10} | {'Match':<5}\")\n",
        "    print(f\"   {'-'*5} | {'-'*10} | {'-'*10} | {'-'*5}\")\n",
        "    \n",
        "    for i in range(len(manually_encoded)):\n",
        "        match = \"✓\" if manually_encoded[i] == dataset_encoded[i] else \"❌\"\n",
        "        print(f\"   {i:<5} | {manually_encoded[i]:<10} | {dataset_encoded[i]:<10} | {match}\")\n",
        "    \n",
        "    # Kiểm tra các token đặc biệt\n",
        "    print(\"\\n7. Kiểm tra các token đặc biệt trong từng từ điển:\")\n",
        "    specials = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<s>\", \"</s>\"]\n",
        "    \n",
        "    print(f\"   {'Token':<10} | {'w2i':<10} | {'i2w lookup':<15}\")\n",
        "    print(f\"   {'-'*10} | {'-'*10} | {'-'*15}\")\n",
        "    \n",
        "    for token in specials:\n",
        "        if token in w2i:\n",
        "            idx = w2i[token]\n",
        "            lookup = i2w[idx] if idx in i2w else \"MISSING\"\n",
        "            print(f\"   {token:<10} | {idx:<10} | {lookup:<15}\")\n",
        "        else:\n",
        "            print(f\"   {token:<10} | {'NOT FOUND':<10} | {'N/A':<15}\")\n",
        "    \n",
        "    # Kiểm tra tính nhất quán của w2i và i2w\n",
        "    print(\"\\n8. Kiểm tra tính nhất quán giữa w2i và i2w:\")\n",
        "    consistent = True\n",
        "    issues = 0\n",
        "    \n",
        "    # Kiểm tra xem mọi entry trong w2i có ánh xạ chính xác trong i2w?\n",
        "    for token, idx in w2i.items():\n",
        "        if idx not in i2w or i2w[idx] != token:\n",
        "            print(f\"   Không nhất quán: w2i['{token}'] = {idx}, nhưng i2w[{idx}] = {i2w.get(idx, 'MISSING')}\")\n",
        "            consistent = False\n",
        "            issues += 1\n",
        "            if issues >= 5:  # Chỉ hiển thị tối đa 5 vấn đề\n",
        "                print(\"   ... và nhiều vấn đề khác ...\")\n",
        "                break\n",
        "    \n",
        "    if consistent:\n",
        "        print(\"   ✓ Không phát hiện vấn đề inconsistency giữa w2i và i2w\")\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f2d7c3c",
      "metadata": {},
      "source": [
        "## 3. Train mô hình CodeBERT - LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d33087c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d33087c",
        "outputId": "18b8f404-1f40-4dae-b62c-fc66bc97824c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "\n",
        "class Code2QueDataset(Dataset):\n",
        "    def __init__(self, xy_list, w2i, tokenizer, i2w=None, max_src_len=128, max_tgt_len=64):\n",
        "        self.data = xy_list\n",
        "        self.w2i = w2i\n",
        "        self.i2w = i2w  # Lưu trữ i2w được truyền vào\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_src_len = max_src_len\n",
        "        self.max_tgt_len = max_tgt_len\n",
        "        self.pad_id = w2i['<pad>']\n",
        "        \n",
        "        # Thêm BOS token nếu chưa có\n",
        "        if '<bos>' not in w2i:\n",
        "            bos_id = len(w2i)\n",
        "            w2i['<bos>'] = bos_id\n",
        "            \n",
        "            # Cập nhật i2w nếu được cung cấp\n",
        "            if self.i2w is not None:\n",
        "                self.i2w[bos_id] = '<bos>'\n",
        "                print(f\"Added BOS token with ID {bos_id} to both w2i and i2w\")\n",
        "            \n",
        "            self.bos_id = bos_id\n",
        "        else:\n",
        "            self.bos_id = w2i['<bos>']\n",
        "            \n",
        "        # Kiểm tra nếu BOS tồn tại trong w2i nhưng không có trong i2w\n",
        "        if self.i2w is not None and w2i['<bos>'] not in self.i2w:\n",
        "            self.i2w[w2i['<bos>']] = '<bos>'\n",
        "            print(f\"Added missing BOS token ID {w2i['<bos>']} to i2w\")\n",
        "\n",
        "        # Kiểm tra và thiết lập EOS token\n",
        "        if '<eos>' in w2i:\n",
        "            self.eos_id = w2i['<eos>']\n",
        "        elif '</s>' in w2i:\n",
        "            self.eos_id = w2i['</s>']\n",
        "        else:\n",
        "            self.eos_id = w2i.get('<eos>', 2)  # Mặc định dùng ID 2 nếu không có\n",
        "\n",
        "        print(f\"Using BOS token ID: {self.bos_id}, EOS token ID: {self.eos_id}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        code_data, question_data = self.data[idx]\n",
        "\n",
        "        # Lấy văn bản gốc của code và question\n",
        "        code_text = code_data[1]\n",
        "        question_tokens = question_data[0]\n",
        "\n",
        "        # Convert question tokens to indices\n",
        "        question_indices = [self.bos_id]  # Thêm BOS token\n",
        "        for token in question_tokens:\n",
        "            if token == \"<eos>\":\n",
        "                question_indices.append(self.eos_id)\n",
        "            elif token in self.w2i:\n",
        "                question_indices.append(self.w2i[token])\n",
        "            else:\n",
        "                question_indices.append(self.w2i[\"<unk>\"])\n",
        "\n",
        "        # Padding/truncating question indices\n",
        "        if len(question_indices) < self.max_tgt_len:\n",
        "            question_indices += [self.pad_id] * (self.max_tgt_len - len(question_indices))\n",
        "        else:\n",
        "            # Đảm bảo token cuối cùng là EOS\n",
        "            question_indices = question_indices[:self.max_tgt_len-1] + [self.eos_id]\n",
        "\n",
        "        return {\n",
        "            'code_text': code_text,\n",
        "            'question_indices': np.array(question_indices)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Hàm collate cho DataLoader - phiên bản tối ưu\n",
        "    \"\"\"\n",
        "    code_texts = [item['code_text'] for item in batch]\n",
        "\n",
        "    # Chuyển đổi list của numpy arrays thành một single numpy array trước khi tạo tensor\n",
        "    question_indices_np = np.stack([item['question_indices'] for item in batch])\n",
        "    question_indices = torch.tensor(question_indices_np, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'code_texts': code_texts,\n",
        "        'question_indices': question_indices\n",
        "    }\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, tokenizer, w2i, i2w,\n",
        "                device, epochs=10, lr=2e-5, checkpoint_dir='checkpoints',\n",
        "                patience=3):\n",
        "    \"\"\"\n",
        "    Huấn luyện mô hình Code2Que với tập validation và learning rate scheduler.\n",
        "    \n",
        "    Args:\n",
        "        model: Mô hình CodeBERTLSTM\n",
        "        train_dataloader: DataLoader cho tập huấn luyện\n",
        "        val_dataloader: DataLoader cho tập validation\n",
        "        tokenizer: CodeBERT tokenizer\n",
        "        w2i: Word-to-index mapping\n",
        "        i2w: Index-to-word mapping\n",
        "        device: Thiết bị để huấn luyện (cuda hoặc cpu)\n",
        "        epochs: Số lượng epochs\n",
        "        lr: Learning rate ban đầu\n",
        "        checkpoint_dir: Thư mục lưu checkpoints\n",
        "        patience: Số epochs chờ khi không có cải thiện validation loss\n",
        "    \"\"\"\n",
        "    # Tạo thư mục checkpoint nếu chưa tồn tại\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    \n",
        "    # Thêm learning rate scheduler\n",
        "    # ReduceLROnPlateau giảm LR khi validation loss không cải thiện\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \n",
        "        mode='min',\n",
        "        factor=0.5,       # Nhân LR với 0.5 khi cần giảm\n",
        "        patience=1,       # Số epochs chờ trước khi giảm LR\n",
        "        verbose=True,     # In ra thông báo khi LR thay đổi\n",
        "        min_lr=1e-6       # LR tối thiểu\n",
        "    )\n",
        "    \n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_samples = 0\n",
        "\n",
        "        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs} [Training]')\n",
        "        for batch in progress_bar:\n",
        "            code_texts = batch['code_texts']\n",
        "            question_indices = batch['question_indices'].to(device)\n",
        "\n",
        "            code_inputs = tokenizer(\n",
        "                code_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128\n",
        "            ).to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=code_inputs.input_ids,\n",
        "                attention_mask=code_inputs.attention_mask,\n",
        "                target_tokens=question_indices,\n",
        "                teacher_forcing_ratio=1\n",
        "            )\n",
        "\n",
        "            loss = compute_loss(outputs, question_indices, pad_id=w2i[\"<pad>\"])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_size = question_indices.size(0)\n",
        "            total_train_loss += loss.item() * batch_size\n",
        "            train_samples += batch_size\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'avg_loss': f'{total_train_loss/train_samples:.4f}',\n",
        "                'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'  # Hiển thị learning rate hiện tại\n",
        "            })\n",
        "\n",
        "        train_loss = total_train_loss / train_samples\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        val_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_dataloader, desc=f'Epoch {epoch+1}/{epochs} [Validation]')\n",
        "            for batch in progress_bar:\n",
        "                code_texts = batch['code_texts']\n",
        "                question_indices = batch['question_indices'].to(device)\n",
        "\n",
        "                code_inputs = tokenizer(\n",
        "                    code_texts,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=128\n",
        "                ).to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=code_inputs.input_ids,\n",
        "                    attention_mask=code_inputs.attention_mask,\n",
        "                    target_tokens=question_indices,\n",
        "                    teacher_forcing_ratio=0.0  # No teacher forcing during validation\n",
        "                )\n",
        "\n",
        "                loss = compute_loss(outputs, question_indices, pad_id=w2i[\"<pad>\"])\n",
        "\n",
        "                batch_size = question_indices.size(0)\n",
        "                total_val_loss += loss.item() * batch_size\n",
        "                val_samples += batch_size\n",
        "\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'avg_loss': f'{total_val_loss/val_samples:.4f}'\n",
        "                })\n",
        "\n",
        "        val_loss = total_val_loss / val_samples\n",
        "\n",
        "        # Cập nhật learning rate dựa trên val_loss\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}:')\n",
        "        print(f'  Train Loss: {train_loss:.4f}')\n",
        "        print(f'  Validation Loss: {val_loss:.4f}')\n",
        "        print(f'  Learning Rate: {current_lr:.2e}')\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),  # Lưu trạng thái scheduler\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'w2i': w2i,\n",
        "            'i2w': i2w,\n",
        "            'lr': current_lr\n",
        "        }, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt'))\n",
        "        \n",
        "        # Early stopping and save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),  # Lưu trạng thái scheduler\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'w2i': w2i,\n",
        "                'i2w': i2w,\n",
        "                'lr': current_lr\n",
        "            }, best_model_path)\n",
        "            print(f\"  New best model saved with validation loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"  Early stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "                \n",
        "    return best_model_path\n",
        "\n",
        "# Đánh giá mô hình bằng các metrics\n",
        "def evaluate_model(model, test_dataloader, tokenizer, w2i, i2w, device):\n",
        "    \"\"\"\n",
        "    Tính corpus-level BLEU và ROUGE trên toàn bộ tập test.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    rouge = Rouge()\n",
        "\n",
        "    all_preds = []  # List[List[str]]\n",
        "    all_refs  = []  # List[List[List[str]]]  mỗi phần tử là list các tham chiếu cho 1 dự đoán\n",
        "\n",
        "    # Đảm bảo BOS token cũng có trong i2w\n",
        "    if '<bos>' in w2i and w2i['<bos>'] not in i2w:\n",
        "        i2w[w2i['<bos>']] = '<bos>'\n",
        "        print(f\"Added missing BOS token ID {w2i['<bos>']} to i2w dictionary\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader, desc='Evaluating'):\n",
        "            code_texts    = batch['code_texts']\n",
        "            reference_ids = batch['question_indices']  # Tensor [batch, tgt_len]\n",
        "\n",
        "            # Tokenize code\n",
        "            code_inputs = tokenizer(\n",
        "                code_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=test_dataloader.dataset.max_src_len\n",
        "            ).to(device)\n",
        "\n",
        "            # Sinh dự đoán\n",
        "            gen_ids, _ = model.generate(\n",
        "                input_ids=code_inputs.input_ids,\n",
        "                attention_mask=code_inputs.attention_mask,\n",
        "                max_length=test_dataloader.dataset.max_tgt_len,\n",
        "                bos_id=w2i['<bos>'],\n",
        "                eos_id=w2i['<eos>']\n",
        "            )\n",
        "\n",
        "            # Chuyển IDs → token strings, loại special tokens\n",
        "            for pred_ids, ref_ids in zip(gen_ids, reference_ids):\n",
        "                pred_tokens = [\n",
        "                    i2w[int(idx)] for idx in pred_ids\n",
        "                    if i2w[int(idx)] not in {'<pad>','<bos>','<eos>','<s>','</s>'}\n",
        "                ]\n",
        "                ref_tokens = [\n",
        "                    i2w[int(idx)] for idx in ref_ids\n",
        "                    if i2w[int(idx)] not in {'<pad>','<bos>','<eos>','<s>','</s>'}\n",
        "                ]\n",
        "                all_preds.append(pred_tokens)\n",
        "                all_refs.append([ref_tokens])  # mỗi ref nằm trong list để corpus_bleu hiểu\n",
        "\n",
        "    # Corpus-level BLEU\n",
        "    corpus_bleu_score = corpus_bleu(\n",
        "        all_refs,\n",
        "        all_preds,\n",
        "        smoothing_function=smoothie\n",
        "    )\n",
        "\n",
        "    # ROUGE: vẫn tính bình thường trung bình trên từng cặp\n",
        "    hyp_texts = [\" \".join(p) for p in all_preds]\n",
        "    ref_texts = [\" \".join(r[0]) for r in all_refs]\n",
        "    rouge_scores = rouge.get_scores(hyp_texts, ref_texts)\n",
        "\n",
        "    # Trung bình ROUGE\n",
        "    avg_rouge = {\n",
        "        metric: {\n",
        "            m: sum(score[metric][m] for score in rouge_scores) / len(rouge_scores)\n",
        "            for m in rouge_scores[0][metric]\n",
        "        }\n",
        "        for metric in rouge_scores[0]\n",
        "    }\n",
        "\n",
        "    print(f\"Corpus BLEU: {corpus_bleu_score:.4f}\")\n",
        "    return {\n",
        "        'bleu': corpus_bleu_score,\n",
        "        'rouge': avg_rouge\n",
        "    }\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Kiểm tra CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Khởi tạo datasets\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "train_dataset = Code2QueDataset(train_xy_list, w2i, tokenizer, i2w=i2w)\n",
        "val_dataset = Code2QueDataset(val_xy_list, w2i, tokenizer, i2w=i2w)\n",
        "test_dataset = Code2QueDataset(test_xy_list, w2i, tokenizer, i2w=i2w)\n",
        "sample = train_dataset[0]\n",
        "question_indices = sample['question_indices']\n",
        "print(question_indices)\n",
        "test_sample_encoding()\n",
        "# Khởi tạo dataloaders\n",
        "batch_size = 16  \n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Khởi tạo model\n",
        "vocab_size = len(w2i)\n",
        "model = CodeBERTLSTM(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=256,\n",
        "    decoder_hidden_dim=256,\n",
        "    decoder_layers=2,\n",
        "    dropout=0.3,\n",
        "    freeze_encoder=True  # Freeze encoder cho quá trình đầu tiên để tránh overfitting\n",
        ").to(device)\n",
        "\n",
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "best_model_path = train_model(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    tokenizer=tokenizer,\n",
        "    w2i=w2i,\n",
        "    i2w=i2w,\n",
        "    device=device,\n",
        "    epochs=4,\n",
        "    lr=1e-4,\n",
        "    checkpoint_dir=MODEL_PATH,\n",
        "    patience=3  # Early stopping patience\n",
        ")\n",
        "\n",
        "# Load best model for evaluation\n",
        "checkpoint = torch.load(os.path.join(MODEL_PATH, 'checkpoint_epoch_1.pt'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Evaluating model...\")\n",
        "metrics = evaluate_model(\n",
        "    model=model,\n",
        "    test_dataloader=test_dataloader,\n",
        "    tokenizer=tokenizer,\n",
        "    w2i=w2i,\n",
        "    i2w=i2w,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d24ac6",
      "metadata": {},
      "source": [
        "## 4. Đánh giá kết quả \n",
        "- Dùng corpus-level BLEU, ROUGE để kiểm tra độ tương đồng từ vựng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f562f17d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results:\n",
            "BLEU: 0.1113\n",
            "ROUGE-1 F1: 0.3420\n",
            "ROUGE-L F1: 0.2844\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluation results:\")\n",
        "print(f\"BLEU: {(metrics['bleu']):.4f}\")\n",
        "print(f\"ROUGE-1 F1: {(metrics['rouge']['rouge-1']['f']):.4f}\")\n",
        "print(f\"ROUGE-L F1: {(metrics['rouge']['rouge-l']['f']):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
